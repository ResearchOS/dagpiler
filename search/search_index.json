{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dagpiler","text":"<p>Compile data processing pipelines from independent packages as a NetworkX DAG with the <code>dagpiler</code> package. <pre><code>pip install dagpiler\n</code></pre></p>"},{"location":"#problem-statement","title":"Problem Statement","text":"<p>It is challenging to integrate data analyses written by other people or organizations into your own data processing pipelines due to the large variation in data analyses and data. Presently, many organizations custom building their data processing pipelines, spending much of their time managing the uninteresting aspects such as file saving/loading, handling dependencies, etc. wasting lots of time re-creating infrastructure that already exists elsewhere.</p> <p>While there are established workflow orchestration tools such as Apache Airflow, they do not focus on being able to share and use data processing pipelines written by others. There is a need for a lightweight, standardized way to define data processing pipelines that can be shared and used by others.</p>"},{"location":"#solution","title":"Solution","text":"<p>The <code>dagpiler</code> package solves the problem of reusing and sharing data analysis pipelines in the same way that modern software development reuses and shares software: by treating data processing pipelines as standalone packages. These packages use TOML files and Python's native packaging system to define and publicly share data processing pipelines. These packages can then be installed using <code>pip install</code> and incorporated by others in their own data processing pipelines via a \"bridging\" mechanism. The compilation process converts TOML files into a NetworkX Directed Acyclic Graph (DAG) that is intended to provide all of the requisite metadata for running the data processing pipeline.</p>"},{"location":"#future-toolkit","title":"Future Toolkit","text":"<p><code>dagpiler</code> is the first of a larger suite of tools that will be developed to support the entire data processing pipeline lifecycle, from dataset creation to data analysis, visualization, and reporting. The goal is to make it easy to share and use data processing pipelines, and to make it easy to integrate data analyses from multiple sources into a single pipeline.</p>"},{"location":"dag_structure/","title":"DAG Structure","text":"<p>Todo</p> <ol> <li>Understand the types of DAG's that this compiler may generate. At the moment, it only puts out furcated (split) graphs. </li> <li>What does a split represent, why do I want it, and how do I trigger it?</li> <li></li> <li> <p>How the DAG is structured</p> <ul> <li>Node naming conventions, and how it relates to TOML</li> <li>Node metadata. Which metadata changes the hash?  </li> </ul> </li> <li> <p>Polyfurcations. What are they, when do they occur, why are they good, and what are the implementation details?</p> </li> </ol>"},{"location":"philosophy/","title":"DAG Design and Philosophy","text":"<p>The Directional Acyclic Graph (DAG) is a data structure (a type of Graph) that at its most fundamental level consists of nodes and directed (Directional) edges, and does not contain any loops (Acyclic). Typically, nodes are objects, and directed edges are the directional relationships between them, such as <code>gas -&gt; car</code> indicating that the car depends on gas. It is the same in this package. Typically, edges are notated using some variant of <code>source -&gt; target</code> (equivalently, <code>(source, target)</code>), which can be read in one of two ways:</p> <ol> <li>The target depends on the source.</li> <li>The source feeds in to the target.</li> </ol> <p>The second, more source-centric interpretation guides the design philosophy of this package. In the context of a workflow orchestration tool, where data flows from a dataset to a final output, I think this makes more sense. In the DAG, nodes are Runnable functions and the edges are the Variables' data flowing between them, the edges look more like <code>Runnable -&gt; Variable</code> which can be read as \"data from this Runnable node flows into this output Variable\". Similarly, <code>Variable -&gt; Runnable</code> indicates that data flows from a Variable and is an input to a Runnable.</p> <p>In this package, edges carry no meaning or metadata, they simply define directional connectivity. All of the metadata is contained in the node properties.</p> <p>At a high level, there are two types of nodes: Runnables, and Variables. Runnables represent things that you run, generally a function that you can call or a script you can run. Categories of Runnables include Process, Stats, and Plot. More on those later. The other node type is Variables, which as you would expect help direct the flow of data. Runnable nodes can only directly connect to Variables, they cannot connect to each other directly. However, Variable nodes can connect to either a Runnable or another Variable. The edges between nodes, whether they be Runnable -&gt; Variable, Variable -&gt; Runnable, or Variable -&gt; Variable, represent the flow of data between steps in a data processing pipeline. There can only be one connection between an output variable node and an input variable node without triggering a split in the DAG. A split is exactly what it sounds like. An exact copy of the subgraph of nodes descended from the current  Runnable node is created, and attached to that same Runnable node.</p>"},{"location":"templates/","title":"Template Structure","text":"<p>Todo</p> <p>Templates for new projects can be created using <code>ros create $package_name</code>. This section defines the structure of the template. What is the directory structure, which files are included, what information needs to be entered in order to share the package? How to integrate with others' packages?</p>"},{"location":"terms/","title":"Glossary","text":""},{"location":"terms/#a-note-on-notation","title":"A note on notation","text":"<p>Throughout the glossary and the docs, you may see notation for file and folder paths that include dollar signs <code>$</code>. Whatever comes after this symbol is intended to be a dynamic variable, e.g. <code>$project_folder</code> will be replaced with the actual folder path for your project.</p>"},{"location":"terms/#dag","title":"DAG","text":"<p>The output of this package is a Directional Acyclic Graph (DAG) consisting of nodes and edges as NetworkX MultiDiGraph object. Nodes can be Runnables or Variables, and edges are the connections between nodes. </p>"},{"location":"terms/#indextoml","title":"index.toml","text":"<p>Recommended to be located at <code>$project_folder/src/$project_name/index.toml</code>. This file contains all of the file paths to all of the files that comprise this package. For maximum flexibility, the only requirement as to the structure of this file is that it consist only of dictionaries (with any degree of nesting for organizational purposes), where each key is a user-defined string, and the values are either a subdictionary, or a file path. No other strings, no numbers, or lists are allowed outside of dictionaries. Relative file paths are preferred for portability. They are relative to the <code>$project_folder/src/$project_name</code> directory, as that is the root directory when the package is installed. Absolute file paths should be used only when needed to access files outside of the project folder.</p>"},{"location":"terms/#pyprojecttoml","title":"pyproject.toml","text":"<p>Recommended to be located at the root of your project folder, <code>pyproject.toml</code> is a type of text file that is Python's default way of providing the metadata needed to share Python packages. This is the only Python-standard .toml file, the rest are custom-defined in this package for the purposes of compiling a DAG from a TOML-based modular package format.</p>"},{"location":"terms/#variable","title":"Variable","text":"<p>Variables are defined within the Runnables where they are used, they do not have their own sections of a TOML file. Variables can be specified as inputs or outputs, and are the primary way that data flows between Runnables.</p> <p>A Variable node in the DAG is an input Variable if its successor is a Runnable, and an output Variable if its predecessor is a Runnable. Input Variables can be any of several types: hard-coded, loading a file, specifying a data object's name or file path, or even unspecified. Output variables do not have these delineations - they are all simply \"outputs\".</p>"},{"location":"terms/#runnable","title":"Runnable","text":"<p>\"Runnable\" is an umbrella term for any node type that is not a Variable, and executes code. The default Runnable types are Process, Plot, PlotComponent, and Summary (in development). There are different types of Runnables because they each require different attributes to function, though some attributes are mandatory and shared between all Runnable types.</p>"},{"location":"terms/#runnable-process","title":"Runnable: Process","text":"<p>The most common type of Runnable. Takes in data, processes it by executing the associated code, and outputs data.</p>"},{"location":"terms/#runnable-plot","title":"Runnable: Plot","text":"<p>Runnable that visualizes data. Takes in data and metadata about the Plot, Axes, and PlotComponent to construct and save the plot. </p> <p>Info</p> <p>Plots themselves do not have code associated with them, and so do not have an <code>exec</code>property. However, PlotComponents do.</p>"},{"location":"terms/#runnable-plotcomponent","title":"Runnable: PlotComponent","text":"<p>Todo</p> <p>Used by Plot-type Runnables to define a single layer of the plot. Executes the plotting functions, while Plot Runnables themselves do not.</p>"},{"location":"terms/#runnable-summary","title":"Runnable: Summary","text":"<p>Todo</p> <p>Responsible for summarizing the data so it can be entered into statistical analysis.</p>"},{"location":"terms/#bridges","title":"Bridges","text":"<p>The mechanism to connect Runnables in different packages. Using similar syntax as in the package's TOML files, the separation of the bridge file allows for the separation of concerns between the package's internal structure and its external connections. This means that a package can be written to reference variables from other packages without knowing anything about those other packages. For more information, see the Bridging page.</p>"},{"location":"Packaging/bridging_packages/","title":"Bridging Packages","text":"<p>When packages are developed independently of one another, by definition they have no way of knowing what inputs or outputs the other provides. Bridging is the mechanism by which independently developed packages are connected together. The bridge name is just an identifier (unique within each package). Sources are the origin of the bridge. Any type of input variable can be entered as the source, see Process. Targets are where the variable is being directed to. Typically, there would either be just one source and multiple targets, or one target and multiple sources.</p> <p>Here is an example of a bridges.toml file:</p> <p><pre><code>[bridge_name]\nsources = [\n    \"package1.runnable1.output1\"\n]\ntargets = [\n    \"package2.runnable1.input1\"\n]\n</code></pre> Excerpts from package 1 and 2's runnable.toml file: <pre><code># package1/src/package1/runnable1.toml\n[runnable1]\ntype = \"process\"\ninputs.input1 = 5\noutputs = [\n    \"output1\"\n]\n\n# package2/src/package2/runnable1.toml\n[runnable1]\ntype = \"process\"\ninputs.input1 = \"?\"\noutputs = [\n    \"output1\"\n]\n</code></pre></p> <p>Bridges will most typically be used to provide an input variable for a variable that is Unspecified in its own package, meaning it relies on other packages to provide that variable. However, any variable from any package can be overriden by being bridged. Other common use cases include providing non-default hard-coded values, or specifying a different file path to load a variable from.</p> <p>Multiple sources leading to one target indicates that the DAG will split at that target, creating a copy of the subgraph of nodes descended from the target. This is useful when you want to run multiple processes on the same data. <pre><code>[bridge_name]\nsources = [\n    \"package1.runnable1.output1\",\n    \"package2.runnable1.output1\"\n]\ntargets = [\n    \"package3.runnable1.input1\"\n]\n</code></pre></p>"},{"location":"Packaging/creating_packages/","title":"Creating New Packages","text":"<p>For any data science project, or when building a data processing pipeline component, first you need to initialize your project.</p> <ol> <li> <p>Create a new directory for your project. <pre><code>mkdir $project_folder\n</code></pre></p> </li> <li> <p>Create a new virtual environment in the project directory and activate it.</p> </li> </ol> <p>Warning</p> <p>For now, the virtual environment MUST be named <code>.venv</code> to work with the dagpiler package.</p> <pre><code>cd $project_folder\npython -m venv .venv\n\nsource .venv/bin/activate # Linux and MacOS\n.venv\\Scripts\\activate # Windows\n</code></pre> <ol> <li> <p>Install the dagpiler package using pip <pre><code>pip install dagpiler\n</code></pre></p> </li> <li> <p>Initialize the project with the <code>dagpiler init</code> command. This creates the folder structure and files needed for the project. <pre><code>dagpiler init\n</code></pre> It will ask you for the following metadata to set up the pyproject.toml and mkdocs.yml files:</p> <ul> <li><code>name</code>: The name of the package (REQUIRED)</li> <li><code>author name</code>: The name of the author (OPTIONAL)</li> <li><code>author email</code>: The email of the author (OPTIONAL)</li> </ul> </li> </ol> <p>Tip</p> <p>If you don't want to provide any metadata here (including the package name), you can provide it later in the pyproject.toml and mkdocs.yml files manually. This step is inteded only to save you time later.</p> <ol> <li> <p>Write the TOML files that define your data processing pipeline components. For more information on the types of TOML files, see the Types of TOML Files page.</p> </li> <li> <p>Compile the TOML files into a Directed Acyclic Graph (DAG) as a NetworkX MultiDiGraph object using the <code>dagpiler compile</code> command line command. This command will check the TOML files for errors and compile them into the DAG.</p> </li> </ol> <p>Warning</p> <p>No matter how you run the <code>compile</code> command, the package name must match a package that has been pip installed in the current folder's virtual environment (.venv).</p> <p>To run the command from Python: <pre><code>from dagpiler import compile_dag\n\npackage_name = \"my_package\"\ndag = compile_dag(package_name)\n</code></pre></p>"},{"location":"Packaging/installing_packages/","title":"Installing Packages","text":"<p>You've found a package you want to use, and now you want to install it! First, you need to put that package in your pyproject.toml file. Depending on where the package is being installed from, there are a few ways to do this using Python's default syntax. For more information than is provided here, check out the linked documentation in the sections below.</p> <p>After updating your <code>pyproject.toml</code> file, you can install all of your package's dependencies by installing your own package using <code>pip install .</code> or <code>pip install -e .</code> from the root directory of your package.</p>"},{"location":"Packaging/installing_packages/#installing-from-pypi","title":"Installing from PyPI","text":"<p>Packages installed from PyPI can be specified with their package name, and optionally version restrictions. See Python docs for details.  <pre><code>[project]\ndependencies = [\n    \"numpy\",\n    \"pandas\"\n]\n</code></pre></p>"},{"location":"Packaging/installing_packages/#installing-from-github","title":"Installing from GitHub","text":"<p>From the Hatch documentation, packages can also be installed directly from GitHub repositories. <pre><code>[project]\ndependencies = [\n    \"dagpiler @ git+https://github.com/ResearchOS/dagpiler\"\n]\n</code></pre></p>"},{"location":"Packaging/installing_packages/#installing-from-a-local-directory","title":"Installing from a local directory","text":"<p>Similarly, dependency packages can be installed from a local directory as well by specifying paths to either the package folder, a wheel (.whl), or source archive (.tar.gz). <pre><code>[project]\ndependencies = [\n    \"dagpiler @ file:///path/to/dependent/package/dagpiler\" # Package folder path\n]\n</code></pre></p>"},{"location":"Packaging/installing_packages/#installing-from-the-internet-other-than-version-control","title":"Installing from the Internet (other than version control)","text":"<p>Finally, packages can also be downloaded from any location on the Internet by specifying the package name and the link to the wheel or source archive. For example: <pre><code>[project]\ndependencies = [\n    \"pytorch @ https://download.pytorch.org/whl/cu102/torch-1.10.0%2Bcu102-cp39-cp39-linux_x86_64.whl\"\n]\n</code></pre></p>"},{"location":"Packaging/plot/","title":"Plot","text":"<p>Unlike Processes, where all of the attributes were used in every Process, Plot Runnables have many attributes, some of which are not required for every Plot, depending on the type of plot you are creating.</p> <p>Plots also consist of one or more axes, and each axes consists of one or more PlotComponents, which are the individual layers of the plot. Each PlotComponent has its own attributes, while the Plot itself has attributes that apply to the entire plot.</p>"},{"location":"Packaging/plot/#plot-attributes","title":"Plot Attributes","text":""},{"location":"Packaging/plot/#required","title":"Required","text":"<p>Every plot must specify at least its type and one axes, and one component in that axes. <pre><code>[plot_name]\ntype = \"plot\"\n\n[plot_names.axes.axes_name]\ncomponent_order = [\n    \"component_name\"\n]\n\n[plot_names.axes.axes_name.components.component_name]\npath = \"path/to/component_name.toml\"\ninputs.input1 = \"runnable1.variable1\"\nprops.prop1 = \"value\" # Overrides the default value for the component\n</code></pre></p>"},{"location":"Packaging/plot/#optional","title":"Optional","text":"<p>Below is all of the optional attributes that can be specified in the <code>[plot_name]</code> table of a Plot Runnable.</p> <pre><code>[plot_name]\nsize = [width, height] # The size of the plot in inches\nplot_backend = \"matlab\" # The plotting backend to use (default is \"matlab\")\n\n# If a movie is being plotted\nmovie.frames.start = 0 # The first index in the variable to plot\nmovie.frames.end = 10 # The last index in the variable to plot\nmovie.frames.step = 1 # The step size between frames\nmovie.frames.speed = 1 # The speed at which to play the movie. 1 is normal speed, 0.5 is half speed, etc.\nmovie.frames.save = false # Whether to save the frames as images (default is false)\n</code></pre> <p>Here are all of the optional attributes that can be specified in the <code>[plot_names.axes.axes_name]</code> table of a Plot Runnable. Note that the <code>x/y/zlim</code> and <code>view</code> attributes can be specified hard-coded or dynamically using variable names. <pre><code>[plot_name.axes.axes_name]\nposition = [rows, columns, index] # The position of the axes in the plot following the matplotlib convention\n# OR\nposition = [left, bottom, width, height] # The position of the axes in the plot following the matplotlib convention\nfont_size = 12 # The font size of the text on the axes\nview = [azimuth, elevation] # The view of the plot in 3D space (default is [0, 90] for 2D plots)\ntitle = \"Axes Title\" # The title of the axes\nxlabel = \"X-axis Label\" # The label for the x-axis\nylabel = \"Y-axis Label\" # The label for the y-axis\nzlabel = \"Z-axis Label\" # The label for the z-axis\nxlim = [min, max] # The limits of the x-axis\nylim = [min, max] # The limits of the y-axis\nzlim = [min, max] # The limits of the z-axis\n</code></pre></p> <p>Finally, here are all of the optional attributes that can be specified in the <code>[plot_names.axes.axes_name.components.component_name]</code> table of a Plot Runnable. <pre><code>[plot_name.axes.axes_name.components.component_name]\nprops.prop1 = \"value\" # Overrides the default value of \"prop1\" for the component\n\n# Example\nprops.LineWidth = 2 # Overrides the default value of \"LineWidth\" for the component\n</code></pre></p>"},{"location":"Packaging/plot_component/","title":"PlotComponent","text":"<p>PlotComponents are the individual layers of a plot. They are used by Plot-type Runnables to define the appearance of the plot. Each PlotComponent has its own attributes, while the Plot itself has attributes that apply to the entire plot.</p> <p>Here is a minimal working example of a PlotComponent TOML file: <pre><code># $project_folder/src/$project_name/path/to/component_name.toml\n[component_name]\ntype = \"component\"\nexec = \"path/to/file.ext::func_name\"\ninputs_order = [\n    \"input1\",\n    \"input2\"\n]\n</code></pre></p> <p>Note that unlike Process Runnables, the inputs are specified in a list, <code>inputs_order</code>, rather than a dictionary. This is because the actual variables being used as inputs are defined in the Plot Runnable, and the PlotComponent only needs to know the order in which they are used.</p> <p>This is a limitation of TOML, as it alphabetizes the input variables in the dictionary, so the order in which they are specified in the TOML file is not preserved. To work around this, the <code>inputs_order</code> list is used to specify the order in which the variables are used in the function.</p> <p>Here are the optional attributes that can be specified in the <code>[component_name]</code> table of a PlotComponent TOML file: <pre><code>[component_name]\nprops.prop1 = \"value\" # Overrides the default value for the component\n\n# Example\nprops.LineWidth = 2\n</code></pre></p> <p>The <code>props</code> dictionary is specific to the plotting function that this PlotComponent executes. To know which properties a particular plotting function accepts, refer to the documentation for that function in its package's documentation.</p>"},{"location":"Packaging/process/","title":"Process Runnables","text":"<p>Minimal working example: <pre><code>[process_name]\ntype = \"process\"\nexec = \"path/to/file.ext:func_name\"\ninputs.input1 = \"runnable1.variable1\"\noutputs = [\n    \"output1\",\n    \"output2\"\n]\n</code></pre></p> <p>Primarily, the focus when creating Process Runnables is on the <code>inputs</code> fields, as there are many different kinds of inputs that can be specified. Below are definitions and examples of several different kinds of supported inputs.</p>"},{"location":"Packaging/process/#inputs","title":"Inputs","text":""},{"location":"Packaging/process/#dynamic","title":"Dynamic","text":"<p>These are the names of variables that were output from a previous Runnable within the same package. If they come from a different package, they should be specified in the <code>bridges.toml</code> file.</p> <p>The dynamic variable names are formatted as <code>runnable_name.variable_name</code>. The <code>runnable_name</code> is the name of the Runnable that produced the variable, and the <code>variable_name</code> is the name of the variable that was produced. It must match the name of the variable in the <code>outputs</code> list from the producing Runnable. For example: <pre><code>[runnable1_name]\ntype = \"process\"\ninputs.input1 = \"runnable2_name.variable1\"\n</code></pre> This specifies that the input variable <code>variable1</code> from <code>runnable2_name</code> should be used as <code>input1</code> in <code>runnable1_name</code>.</p> <p>Warning</p> <p>If you specify a variable from a Runnable in a different package in the <code>inputs</code> field, the compiler will not be able to find it and will raise an error.</p> <p>Tip</p> <p>You can specify to use only part of a dynamic variable by including the same slicing syntax used in Python dicts and numpy arrays. For example, <code>runnable1.variable1[\"key\"]</code> will only use the value associated with the key <code>\"key\"</code> from the variable <code>variable1</code> produced by <code>runnable1</code>.</p>"},{"location":"Packaging/process/#unspecified","title":"Unspecified","text":"<p>Indicates that this input does not come from a Runnable within this package. These inputs must be specified in the <code>bridges.toml</code> file to run the package. <pre><code>inputs.input1 = \"?\"\n</code></pre></p>"},{"location":"Packaging/process/#load-file","title":"Load File","text":"<p>For a variety of reasons, variables are often loaded from files. Providing the <code>__load__</code> keyword and the relative file path indicates that this variable should be loaded from the provided file. The file path should be relative to the <code>$project_folder/src/$project_name</code> directory of the package. <pre><code>inputs.input1.__load__ = \"path/to/file.ext\"\n</code></pre></p>"},{"location":"Packaging/process/#hard-coded","title":"Hard-Coded","text":"<p>Variables can be specified directly within the TOML file. This is useful for relatively simple variables that can be typed by hand. The hard-coded values can be any valid TOML data type, including integers, floats, strings, lists, and dictionaries. <pre><code>inputs.input1 = 42 # An integer\ninputs.input2 = \"string\" # A string\ninputs.input3 = [1, 2, 3] # A list\n</code></pre></p> <p>Bug</p> <p>Currently, if the hard-coded value is a string that contains a period (\".\") the compiler will think that it is a dynamic variable. In the future, support for escaping periods using \"\\.\" will be addded.</p> <p>Also note that if your hard-coded value is a dictionary with a key matching one of the reserved keys defined here (e.g. <code>__load__</code>), the compiler will raise an error.</p>"},{"location":"Packaging/process/#data-object-name","title":"Data Object Name","text":"<pre><code># The general form of the data object name syntax\ninputs.input1.__data_object_name__ = \"DataObject\" \n\n# In the case where a Subject data object's names is an input.\ninputs.input1.__data_object_name__ = \"Subject\" \n</code></pre>"},{"location":"Packaging/process/#data-object-file-path","title":"Data Object File Path","text":"<pre><code># The general form of the data object file path syntax\ninputs.input1.__data_object_file_path__ = \"DataObject\"\n\n# In the case where a Subject data object's file path is an input.\n# Typically used at the beginning of the pipeline to load the data.\ninputs.input1.__data_object_file_path__ = \"Subject\"\n</code></pre>"},{"location":"Packaging/process/#outputs","title":"Outputs","text":"<p>Outputs are specified as a list of strings. The order of the strings in the list is important, as it determines the order of the outputs. The order of the outputs is important because it is used to identify the data with the variable names.</p> <pre><code>outputs = [\n    \"output1\",\n    \"output2\"\n]\n</code></pre>"},{"location":"Packaging/publishing_packages/","title":"Publishing Packages","text":"<p>You've created a set of TOML files, you've compiled them to a DAG, maybe you've even leveraged pre-existing packages by bridging your package with theirs, and you're ready to share your package with the world. </p>"},{"location":"Packaging/publishing_packages/#formatting","title":"Formatting","text":"<p>The packaging format described here is inteded to be flexible enough to work with projects of all sizes using the provided tools. Recall that when creating a package, the <code>dagpiler init</code> command will create the proper folder structure and files for you.</p>"},{"location":"Packaging/publishing_packages/#package-folder-structure","title":"Package Folder Structure","text":"<p>Verify that your package follows the expected folder structure.</p> <pre><code>root/\n\u251c\u2500\u2500 .venv/ # created by the user with python -m venv .venv\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 $project_name/\n\u2502   \u2502   \u251c\u2500\u2500 index.toml # Package metadata\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_main.py\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 CONTRIBUTING.md\n</code></pre> <p>Use the provided tools to check that your package matches the required format.</p> <p>Todo</p> <p>A command line tool will ensure that the above folder structure is adhered to, including a <code>docs</code> and <code>tests</code> folder.</p> <p>Once your package is in the proper format and fully functioning, there are multiple ways to share your package with the world.</p>"},{"location":"Packaging/publishing_packages/#1-pypi","title":"1. PyPI","text":"<p>The Python Packaging Authority maintains the Python Packaging Index (PyPI), which is where the majority of Python packages reside. Publishing your package to PyPI is the most common way to share your package with the world. Below are the steps to publish your package to PyPI:</p> <ol> <li> <p>Create an account on PyPI</p> </li> <li> <p>Create a distribution package</p> <ul> <li>Ensure that you have filled out the <code>pyproject.toml</code> file with the necessary metadata.</li> </ul> </li> <li> <p>Upload your package to PyPI</p> </li> <li> <p>Publish your documentation</p> <ul> <li>Run <code>mkdocs gh-deploy</code> to publish your documentation to GitHub Pages</li> </ul> </li> </ol> <p>Others can then Install your package from PyPI</p>"},{"location":"Packaging/publishing_packages/#2-github-or-other-online-version-control","title":"2. GitHub (or Other Online Version Control)","text":"<p>If your package is publicly visible and hosted in an online version control platform such as GitHub or another service, you can simply leave it there! Others can <code>pip install</code> directly from your GitHub repository (see how to install packages from GitHub). It's always a good idea to test from another computer that your package can be successfully installed and run.</p>"},{"location":"Packaging/publishing_packages/#3-other-online-repositories","title":"3. Other Online Repositories","text":"<p>There are other locations online that can host your package besides version control platforms, such as the Open Science Foundation, Zenodo, etc. To share your package there, you must include either a wheel or source archive of your package, ideally both.</p>"},{"location":"Packaging/toml_files/","title":"TOML Configuration Files","text":"<p>TOML (Tom's Obvious Minimal Language) is a configuration file syntax that defines a format for human and machine-readable structured plain text. I selected it for this project because it's just as full featured as JSON and YAML, and has multiple ways to represent the same dictionaries, unlike JSON and YAML (which I find helpful). Due to negligible indentation, TOML is very robust and easy to work with. Its primary downside is that it has not been around for as long as YAML or JSON, and so not every language has an existing TOML parser (though Python, MATLAB, and R all do). I take care to keep the TOML files in this package as simple as possible, and to avoid using any advanced features, though this may occasionally lead to some repetition.</p> <p>Tip</p> <p>This page is focused on how to write and use the TOML files. For a more high-level introduction to the concepts on this page, check out the Glossary.</p>"},{"location":"Packaging/toml_files/#pyprojecttoml","title":"pyproject.toml","text":"<p>Python relies on pyproject.toml files to specify the metadata for publishing packages (see Python docs for details). For use with the <code>dagpiler</code> package, a minimal default file structure is provided below. The contents of this file are created when you run <code>dagpiler init</code> in a new project folder. <pre><code># $project_folder/pyproject.toml\n[build-system]\nrequires = ['hatchling']\nbuild-backend = 'hatchling.build'\n\n[project]\nname = \"your_package_name\"\nversion = '0.1.0'\ndescription = 'Your package description'\nauthors = [{name = \"Author Name\", email =\"author@email.com\"}]\ndependencies = [\n    \"dagpiler\"\n]\n</code></pre></p>"},{"location":"Packaging/toml_files/#indextoml","title":"index.toml","text":"<p>This file specifies the paths to all of the other files in the package. For your package to work, this file must be located at <code>$project_folder/src/$project_name/index.toml</code>. It can contain arbitrarily nested dictionaries, but every value must be a file path or list of file paths. Relative file paths are preferred for portability. The root directory for the relative file paths is the directory containing the index.toml file, <code>$project_folder/src/$project_name</code>, as that is the root directory when the package is installed via <code>pip</code>.</p> <p>Tip</p> <p>The index.toml file is the only file that is required to be in a specific location. The rest of the files can be located anywhere as long as the paths are correctly specified in the index.toml file.</p> <p>Tip</p> <p>Try to keep all of the package's files within the <code>$project_folder/src/$project_name</code> directory so that <code>index.toml</code> can reference them using relative paths. Use absolute paths to files outside of this folderonly when necessary, as this is not portable.</p>"},{"location":"Packaging/toml_files/#examples","title":"Examples","text":"<p>The simplest <code>index.toml</code> files are just one key-value pair, where the key can be any string and the value is a file path. For example: <pre><code># $project_folder/src/$project_name/index.toml\npackage_file = \"path/to/package_file.toml\"\n</code></pre> In larger packages with more files, more organization becomes useful. For example, categorizing paths by type: <pre><code># $project_folder/src/$project_name/index.toml\nprocesses = [\n    \"path/to/process1.toml\",\n    \"path/to/process2.toml\"\n]\nplots = [\n    \"path/to/plots1.toml\"\n]\n</code></pre></p>"},{"location":"Packaging/toml_files/#special-keys","title":"Special keys","text":"<p>bridges: The files that connect the current package to other packages. It is a file path or list of file paths. <pre><code># $project_folder/src/$project_name/index.toml\nrunnables = [\n    \"path/to/runnables1.toml\",\n    \"path/to/runnables2.toml\"\n]\nbridges = \"path/to/bridges.toml\"\n</code></pre></p>"},{"location":"Packaging/toml_files/#runnablestoml","title":"runnables.toml","text":"<p>The main contents of a package reside in its 1+ runnables' .toml files, of which there are multiple types. Every type of runnable needs to specify a <code>type</code> attribute, and most will have <code>inputs</code> and/or <code>outputs</code>. Any custom attributes that the user provides in addition to the built-in ones are stored in the nodes, but are not used by the compiler.</p> <p>Below are outlines of the different types of builtin Runnable types. For more information on a specific type, see that Runnable's page.</p>"},{"location":"Packaging/toml_files/#process","title":"Process","text":"<p>Process type runnables are the most frequent runnable type. They process and transform data, and are the only Runnable type that has output variables. Inputs are identified by name, similar to keyword arguments available in most languages. As multiple outputs are typically identified by their ordering, output variables are specified in a list (in the same order that they are output).</p> <pre><code>[runnable_name]\ntype = \"process\"\ninputs.input1 = \"runnable1.variable1\"\noutputs = [\n    \"output1\",\n    \"output2\"\n]\n</code></pre>"},{"location":"Packaging/toml_files/#plot","title":"Plot","text":"<p>Plot type runnables are exactly what they sound like - they plot and visualize data.</p> <pre><code>[runnable_name]\ntype = \"plot\"\ninputs.input1 = \"runnable1.variable1\"\n</code></pre>"},{"location":"Packaging/toml_files/#summary","title":"Summary","text":"<p>Summary type runnables summarize the data.</p> <pre><code>[runnable_name]\ntype = \"summary\"\ninputs.input1 = \"runnable1.variable1\"\n</code></pre>"},{"location":"Packaging/toml_files/#bridgestoml","title":"bridges.toml","text":"<p>Bridges are the mechanism by which independently developed packages are connected together. The bridge name is just an identifier (unique within each package). Sources are the origin of the variable being bridged, and targets are where the variable is being directed to. Typically, there would either be just one source and multiple targets, or one target and multiple sources.</p> <p>Most projects just need one of these bridges files, althouh multiple bridges files are supported. If you find yourself with many bridges, consider splitting the package up into smaller packages.</p> <p>Here is a basic example bridges.toml file: <pre><code>[bridge_name]\nsources = [\n    \"package1.runnable1.output1\"\n]\ntargets = [\n    \"package2.runnable1.input1\"\n]\n</code></pre></p> <p>Note that each entry contains the package name, which is not included in the package's runnables.toml files because the referenced runnables are assumed to be located within the same package. When bridging, the package name must be specified explicitly to resolve potential naming conflicts between packages.</p>"},{"location":"Packaging/toml_files/#one-source-multiple-targets","title":"One source, multiple targets","text":"<p>In this case, one output variable is being used as an input to multiple runnables. This is a common practice, as there are often computed variables that need to be used by multiple functions further along the pipeline.</p>"},{"location":"Packaging/toml_files/#one-target-multiple-sources","title":"One target, multiple sources","text":"<p>In this case, one input variable is receiving data from multiple sources, triggering a polyfurcation of the DAG, with one branch per input variable. Most commonly this would happen with Plot and Summary runnables, to reuse the same runnable to plot or summarize multiple variables, though it is used with Process runnables as well.</p> <p>In the below example, two variables are both being connected to the input variable for a Summary runnable. <pre><code>[summaries]\nsources = [\n    \"package1.runnable1.variable1\",\n    \"package1.runnable2.variable1\"\n]\ntargets = [\n    \"package2.summary1.data\"\n]\n</code></pre></p>"},{"location":"Packaging/toml_files/#multiple-targets-multiple-sources","title":"Multiple targets, multiple sources","text":"<p>Todo</p> <p>Currently unsupported and will raise an error, though in the future I aim to support this. It will be treated as though it were a series of N bridges with one target and multiple sources, where N is the number of targets. Therefore, each source will be applied to each target</p>"},{"location":"dev_blog/","title":"Learning in Public","text":"<p>This is a place for me to document my thoughts and learnings as I develop. It may or may not be useful to anyone else, but the act of writing helps me to solidify my understanding of a topic.</p>"},{"location":"dev_blog/2024-10-02/","title":"October 4, 2024","text":""},{"location":"dev_blog/2024-10-02/#high-level-dag-compilation-design","title":"High-level DAG compilation design","text":"<p>To compile a DAG representing a data processing pipeline from a set of configuration files, we need to perform three steps at the highest level (at least I think it's the highest level):</p> <ol> <li> <p>Read the configuration files</p> </li> <li> <p>Clean &amp; validate the configuration files' data</p> </li> <li> <p>Construct the DAG from that data</p> </li> </ol> <p>Here's a figure of the workflow. The config file paths are read, their data returned as dicts. After validation and cleaning, the data is returned as instances of custom classes. Finally, these classes are inputs to constructing the DAG. </p> <p>Note that this seems to only be the workflow for just one package's configuration files. But an important part of this whole process is managing dependencies of other projects. For that, we need a mechanism to list and install dependencies. The pyproject.toml file is Python's modern mechanism to do exactly that. It replaces older mechanisms using setuptools.</p> <p>The problem this package needs to solve is that while the pyproject.toml file is readily available to the person developing the package, it is not typically installed along with the package when someone else <code>pip install</code>'s it. How can I resolve this?</p>"},{"location":"dev_blog/2024-10-02/#option-1-include-the-pyprojecttoml-file-in-the-distributed-package","title":"Option 1: Include the pyproject.toml file in the distributed package.","text":"<p>To include the pyproject.toml file in the distributed package, modify the pyproject.toml file to contain the following: <pre><code>[tool.hatch.build]\ninclude = [\n    \"pyproject.toml\",\n    \"src/package_dag_compiler/**\"\n]\n</code></pre> The idea is that I could loop through each subfolder in the virtual environment folder, checking for and reading each folder's pyproject.toml files to find all of the dependencies for a specific package. But I think Python and package versioning presents a problem, unless I want to loop through all of the folders.</p>"},{"location":"dev_blog/2024-10-02/#side-detour-learning-about-virtual-environment-folder-structure","title":"Side detour: Learning about virtual environment folder structure","text":"<p>Using Python's venv module, the (abbreviated) folder structure is:  <pre><code>venv/\n    bin/\n        activate\n    include/\n    lib/\n        python3.X/\n            site-packages/\n                package_name/\n    pyvenv.cfg\n</code></pre></p> <p>Looking at the natsort Python package, I am learning a bit about how these packages are typically structured. The package is installed in the <code>site-packages</code> folder, and the package's source code is in the <code>natsort</code> folder. The <code>natsort</code> folder contains the <code>__init__.py</code> file, which is the package's entry point for import. The <code>natsort</code> folder also contains the <code>__main__.py</code> file, which is the package's entry point when running the package as a script. Looking at my own package, the package structure is root/src/package_name, but when installed, the package structure is root/site-packages/package_name.</p> <p>Therefore, in the index.toml files, the relative file paths should be relative to the src/package_name folder, because that's the root folder when the package is installed. Therefore, the pyproject.toml may not actually need to have a path to the index.toml at all! I'll just assume that the index.toml is located in src/package_name.</p> <p>But then the question is, without the inclusion of the pyproject.toml file, how do I know what the dependencies are? I think I need to include the pyproject.toml file in the distributed package instead of making people list dependencies in a separate file.</p> <p>Maybe the better format is just for the index.toml file to contain the dependencies AND the paths to the configuration files? That way, the pyproject.toml file is not needed at all.</p> <p>So, the index.toml file should contain the following: <pre><code>dependencies = [] # Indicates that there are no dependencies\nbridges = [] # Indicates that there are no bridges\npaths = [\n    \"config_file1.toml\",\n    \"config_file2.toml\"\n]\n</code></pre></p> <p>It also appears that whether or not I include the following dictates whether the package's sdist includes the \"src\" folder. E.g. if the below is included in the pyproject.toml, and I have \"src/package_name/test.py\", then when the sdist is created, it'll be \"site-packages/package_name/test.py\". If the below is not included, then the sdist and project directory folders will exactly match. It seems that the contents of this line are reflected in the dist/package_name.version.tar.gz file, but not in the installed package. <pre><code>[tool.hatch.build]\ninclude = [\n    \"src/package_dag_compiler/**\"\n]\n</code></pre></p> <p>I am also learning a valuable lesson about the difference between <code>sdists</code> (compressed .py files) and <code>wheels</code> (compiled binaries of the package's code). From the Python Packaging User Guide:</p> <ol> <li> <p>Do I need both sdists and wheels? </p> <ul> <li>\"For pure-Python packages, the difference between sdists and wheels is less marked. There is normally one single wheel, for all platforms and Python versions. Python is an interpreted language, which does not need ahead-of-time compilation, so wheels contain .py files just like sdists.\"</li> </ul> </li> <li> <p>What's in a wheel?</p> <ul> <li>\"With that being said, there are still important differences between sdists and wheels, even for pure Python projects. Wheels are meant to contain exactly what is to be installed, and nothing more. In particular, wheels should never include tests and documentation, while sdists commonly do.\"</li> </ul> </li> <li> <p>Are wheels needed for pure Python projects?</p> <ul> <li>\"At a glance, you might wonder if wheels are really needed for \u201cplain and basic\u201d pure Python projects. Keep in mind that due to the flexibility of sdists, installers like pip cannot install from sdists directly \u2013 they need to first build a wheel, by invoking the build backend that the sdist specifies (the build backend may do all sorts of transformations while building the wheel, such as compiling C extensions). For this reason, even for a pure Python project, you should always upload both an sdist and a wheel to PyPI or other package indices. This makes installation much faster for your users, since a wheel is directly installable. By only including files that must be installed, wheels also make for smaller downloads.\"</li> </ul> </li> </ol>"},{"location":"dev_blog/2024-10-02/#editable-installs","title":"Editable installs","text":"<p>This detour has turned into a deep dive on Python packaging, which is helpful. I discovered that essentially, by default the /src/project_name folder is treated as the root folder when the package is built and when it is installed, with the exception that the sdist build has all files if not specified.</p> <p>The editable installs work by putting a direct_urls.json file in the site-packages/package_name folder. Sometimes it feels finicky, switching between editable and non-editable installs, but it seems to work.</p>"},{"location":"dev_blog/2024-10-03/","title":"October 3, 2024","text":"<p>Discovered how Hatch (Python's recommended build backend) determines which files to include: https://hatch.pypa.io/latest/plugins/builder/wheel/#default-file-selection. For my purposes, the relevant idea is that it'll include all files in the src//init.py folder, but the init.py file must be present. <p>These can be overriden using the [tool.hatch.build.targets.wheel] table.</p>"},{"location":"dev_blog/2024-10-05/","title":"October 5, 2024","text":"<p>Today I've been testing what I've written so far, which is reading in the files and constructing the DAG for one package. I am now trying to incorporate a second package, which led me a question about how to specify a local dependency in pyproject.toml.</p> <p>Here is the relevant documentation: https://peps.python.org/pep-0440/#direct-references and https://hatch.pypa.io/latest/config/metadata/#allowing-direct-references</p> <p>The syntax for adding a package from disk is: <pre><code>[tool.hatch.metadata]\nallow-direct-references = true\n\n[project]\ndependencies = [\n    \"package_name @ file:///path/to/folder/containing/dependency\"\n]\n</code></pre></p>"},{"location":"dev_blog/2024-10-14/","title":"October 14, 2024","text":"<p>Struggling today with the conventions that I should follow in a couple areas of design.</p>"},{"location":"dev_blog/2024-10-14/#first-question","title":"First Question","text":"<p>First, as I was prototyping the Plot type Runnables, I realized that it may be easier to name the table defining each Runnable by the Runnable's type rather than its name: <pre><code>[[plot]]\nname = \"test plot\"\n</code></pre> Rather than <pre><code>[\"test plot\"]\ntype = \"process\"\n</code></pre></p> <p>At first I shied away from the <code>[[plot]]</code> syntax, until I realized that it does in fact translate into JSON just fine. It has the benefit of only having the runnable's name located in one place. Plot type Runnables need multiple subtables, e.g.: <pre><code>[\"test plot\"]\ntype = \"plot\"\n\n[\"test plot\".axes.ax1]\nfield = value\n</code></pre> In this example, the \"test plot\" name must be repeated twice. If I use the <code>[[plot]]</code> syntax, I can avoid this repetition. Is it less readable? I'm not sure. I'll have to think about it. <pre><code>[[plot]]\nname = \"test plot\"\n\n[[plot.ax1]]\nfield = value\n</code></pre></p>"},{"location":"dev_blog/2024-10-14/#second-question","title":"Second Question","text":"<p>The other question is what exactly to do with any extraneous attributes that are provided in the Runnable's table. I've decided that they will be added to the node's attributes so they're visible as would be expected, but these attributes will not be used in any way by the Runnable. Maybe as more functionality is added with other packages, I'll get a sense of what to do with these attributes. For now, I'll just ignore them.</p>"}]}